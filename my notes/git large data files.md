# Git Strategy & Multi-Computer Architecture Analysis

## ğŸš¨ Current Situation Analysis

### What's Happening Now:
- **Repository Size**: ~21MB of tracked data files (should be ~0MB)
- **Git Operations**: Slower than optimal due to large tracked files
- **Multi-Computer Issue**: New computers get NO data files (only code)

### Current Tracked Data Files (PROBLEM):
```
âŒ historical g spread/g_ts.parquet (7.8MB)
âŒ historical g spread/bond_z.parquet (1.3MB)  
âŒ portfolio/portfolio.parquet (1.2MB)
âŒ runs/combined_runs.parquet (5.9MB)
âŒ runs/run_monitor.parquet (157KB)
âŒ universe/universe.parquet (4.7MB)
âŒ db/trading_analytics_backup_*.db (70MB)
âŒ Plus multiple large CSV files (24MB+)
```

## ğŸ“Š .gitignore Analysis

### âœ… EXCELLENT EXCLUSIONS (Fast Git Operations):
- **Large Data Files**: `*.parquet`, `*.csv`, `*.xlsx`, `*.db` (excluded)
- **Data Directories**: `raw/`, `processed/`, `data/` (excluded)
- **Database Files**: `*.db`, `*.sqlite` (excluded)
- **Generated Content**: `__pycache__/`, logs, temp files (excluded)

### âš ï¸ CURRENT ISSUES:
- **Some parquet files ARE tracked** (6 files totaling ~21MB)
- **Database backup file tracked** (70MB `trading_analytics_backup_20250718_125410.db`)
- **Large CSV files tracked** (24MB `bond_g_sprd_wide.csv`)

## ğŸš¨ CRITICAL IMPLICATIONS FOR MULTI-COMPUTER USAGE:

### 1. Data Architecture Problems:
```
âŒ PROBLEM: When you clone on another computer, you'll get:
- NO raw data files (excluded by .gitignore)
- NO processed data files (excluded by .gitignore) 
- NO database files (excluded by .gitignore)
- ONLY code and configuration
```

### 2. What Happens on New Computer:
```bash
git clone <your-repo>
# Result: Empty data directories, no database, no parquet files
# Your pipeline will FAIL because it expects data files
```

### 3. Version Control Issues:
```bash
# You can't revert to previous data states
# You can't track data changes over time
# You can't reproduce exact results from specific commits
```

## ğŸ’¡ RECOMMENDED ARCHITECTURE FOR MULTI-COMPUTER USAGE:

### Option A: Data Pipeline Architecture (RECOMMENDED)

#### Structure:
```
ğŸ“ Repository (Code Only - Fast Git)
â”œâ”€â”€ ğŸ“ src/                    # Code (tracked)
â”œâ”€â”€ ğŸ“ config/                 # Configuration (tracked)
â”œâ”€â”€ ğŸ“ scripts/                # Data generation scripts (tracked)
â”œâ”€â”€ ğŸ“ data_sources/           # Data source definitions (tracked)
â”‚   â”œâ”€â”€ ğŸ“„ portfolio_sources.yaml
â”‚   â”œâ”€â”€ ğŸ“„ universe_sources.yaml
â”‚   â””â”€â”€ ğŸ“„ g_spread_sources.yaml
â”œâ”€â”€ ğŸ“ schemas/                # Data schemas (tracked)
â”‚   â”œâ”€â”€ ğŸ“„ portfolio_schema.json
â”‚   â”œâ”€â”€ ğŸ“„ universe_schema.json
â”‚   â””â”€â”€ ğŸ“„ g_spread_schema.json
â””â”€â”€ ğŸ“ data/                   # Data (gitignored)
    â”œâ”€â”€ ğŸ“ raw/
    â”œâ”€â”€ ğŸ“ processed/
    â””â”€â”€ ğŸ“ database/
```

#### Benefits:
- âœ… **Ultra-fast git operations** (commits/pushes in seconds)
- âœ… **Version control for code only**
- âœ… **Reproducible data pipelines**
- âœ… **Multi-computer friendly**

#### Implementation:
```python
# config/data_sources.yaml
data_sources:
  portfolio:
    raw_path: "C:/Users/Eddy/YTM Capital Dropbox/Data/portfolio/raw/"
    processed_path: "C:/Users/Eddy/YTM Capital Dropbox/Data/portfolio/processed/"
    schema: "schemas/portfolio_schema.json"
  
  universe:
    raw_path: "C:/Users/Eddy/YTM Capital Dropbox/Data/universe/raw/"
    processed_path: "C:/Users/Eddy/YTM Capital Dropbox/Data/universe/processed/"
    schema: "schemas/universe_schema.json"
  
  g_spread:
    raw_path: "C:/Users/Eddy/YTM Capital Dropbox/Data/g_spread/raw/"
    processed_path: "C:/Users/Eddy/YTM Capital Dropbox/Data/g_spread/processed/"
    schema: "schemas/g_spread_schema.json"
```

### Option B: Sample Data Architecture

#### Structure:
```
ğŸ“ Repository (Code + Sample Data)
â”œâ”€â”€ ğŸ“ src/                    # Application code
â”œâ”€â”€ ğŸ“ sample_data/            # Small sample datasets (tracked)
â”‚   â”œâ”€â”€ ğŸ“„ sample_portfolio.parquet (1MB - 100 records)
â”‚   â”œâ”€â”€ ğŸ“„ sample_universe.parquet (1MB - 500 records)
â”‚   â””â”€â”€ ğŸ“„ sample_g_spread.parquet (1MB - 1000 records)
â””â”€â”€ ğŸ“ data/                   # Full data (gitignored)
```

#### Benefits:
- âœ… **New computers get working sample data**
- âœ… **Fast git operations** (only small samples tracked)
- âœ… **Easy testing and development**

## ğŸ”§ IMMEDIATE ACTIONS REQUIRED:

### 1. Remove Large Files from Git Tracking
```bash
# Remove large files from git history (but keep locally)
git rm --cached "historical g spread/g_ts.parquet"
git rm --cached "historical g spread/bond_z.parquet"
git rm --cached "portfolio/portfolio.parquet"
git rm --cached "runs/combined_runs.parquet"
git rm --cached "runs/run_monitor.parquet"
git rm --cached "universe/universe.parquet"
git rm --cached "db/trading_analytics_backup_*.db"

# Commit the removal
git commit -m "Remove large data files from tracking for fast git operations"
```

### 2. Create Data Source Configuration
```yaml
# config/data_sources.yaml
data_sources:
  portfolio:
    raw_files: ["portfolio/raw data/*.xlsx"]
    processed_file: "portfolio/processed data/portfolio.parquet"
    schema_version: "1.0"
    
  universe:
    raw_files: ["universe/raw data/*.xlsx"]
    processed_file: "universe/universe.parquet"
    schema_version: "1.0"
    
  g_spread:
    raw_files: ["historical g spread/raw data/g_ts.parquet"]
    processed_file: "historical g spread/bond_z.parquet"
    schema_version: "1.0"
```

### 3. Create Data Generation Scripts
```python
# scripts/generate_sample_data.py
def create_sample_datasets():
    """Create small sample datasets for testing"""
    # Generate 1MB sample files with 100-1000 records each
    pass
```

## ğŸš€ MULTI-COMPUTER SETUP PROCESS:

### For New Computer:
```bash
# 1. Clone repository (fast - no large files)
git clone <your-repo>
cd work_supa

# 2. Install dependencies
poetry install

# 3. Configure data paths for your computer
# Edit config/data_sources.yaml with local paths

# 4. Run data pipeline to generate all data
poetry run python db_pipe.py --full-reset

# 5. Verify everything works
poetry run python -m pytest test/
```

### For Data Updates:
```bash
# 1. Update raw data files in external storage
# 2. Run pipeline to regenerate processed data
poetry run python db_pipe.py --update-data

# 3. Commit code changes only
git add .
git commit -m "Update data processing logic"
git push
```

## ğŸ“Š PERFORMANCE IMPACT:

### Current (With Large Files):
- **Commit Time**: 30-60 seconds
- **Push Time**: 2-5 minutes
- **Clone Time**: 5-10 minutes
- **Repository Size**: ~21MB

### After Optimization:
- **Commit Time**: 1-3 seconds
- **Push Time**: 10-30 seconds
- **Clone Time**: 30-60 seconds
- **Repository Size**: ~2MB

## ğŸ”„ VERSION CONTROL STRATEGY:

### Code Versioning:
- âœ… Track all code changes
- âœ… Track configuration changes
- âœ… Track schema changes
- âœ… Track sample data changes

### Data Versioning:
- âŒ Don't track large data files
- âœ… Track data schemas and validation rules
- âœ… Track data processing logic
- âœ… Track data source definitions

### Reproducibility:
- âœ… All data can be regenerated from raw sources
- âœ… Processing logic is version controlled
- âœ… Schemas are version controlled
- âœ… Configuration is version controlled

## ğŸ¯ NEXT STEPS:

1. **Immediate**: Remove large files from git tracking
2. **Short-term**: Create data source configuration
3. **Medium-term**: Implement sample data generation
4. **Long-term**: Set up external data storage strategy

