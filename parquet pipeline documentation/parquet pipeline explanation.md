# ðŸŽ¯ COMPREHENSIVE PIPELINE ARCHITECTURE DOCUMENTATION
**Version 2.5 - Production Ready with Force Full Refresh Enhancement**  
*Last Updated: 2025-07-19*

## Table of Contents
1. [System Overview](#system-overview)
2. [Architecture Components](#architecture-components)
3. [Pipeline Stages](#pipeline-stages)
4. [Data Flow](#data-flow)
5. [Force Full Refresh System](#force-full-refresh-system)
6. [Configuration Management](#configuration-management)
7. [Execution Orchestration](#execution-orchestration)
8. [Data Processing Details](#data-processing-details)
9. [Analysis & Validation](#analysis--validation)
10. [Error Handling & Recovery](#error-handling--recovery)
11. [Monitoring & Logging](#monitoring--logging)
12. [CLI Interface](#cli-interface)
13. [File Structure](#file-structure)
14. [Performance & Optimization](#performance--optimization)
15. [Dependencies & Relationships](#dependencies--relationships)

---

## System Overview

The **Trading Analytics Pipeline System** is a comprehensive data processing and analysis framework designed for financial market data. It orchestrates multiple data sources, processes them into standardized Parquet formats, and provides detailed analytics and validation capabilities.

### Core Purpose
- **Data Ingestion**: Process raw data from multiple sources (Excel, CSV, databases)
- **Standardization**: Convert all data to Parquet format for optimal performance
- **Validation**: Comprehensive CUSIP validation across all data sources
- **Analysis**: Detailed statistical analysis and data quality reporting
- **Orchestration**: Coordinated execution with dependency management
- **Force Refresh**: Complete regeneration of all parquet files from raw data

### Key Features âœ¨
- **ðŸ”„ Force Full Refresh**: Complete regeneration capability for Git version switches
- **âš¡ Parallel Processing**: Optimized concurrent execution with dependency management
- **ðŸ” Comprehensive Analysis**: DataFrame analysis and CUSIP validation across all tables
- **ðŸ§¹ Log Management**: Automatic log cleanup and maintenance
- **ðŸ“Š Data Quality**: Multi-level validation including orphaned CUSIP detection
- **ðŸŽ¯ Production Ready**: Robust error handling and recovery mechanisms

### Current System Capacity
- **Processing Rate**: 7,000+ records/second sustained
- **Database Size**: 50-70 MB (optimized with indexes)
- **Total Records**: ~188,398 across all tables
- **Pipeline Duration**: 4-6 minutes for full refresh
- **Memory Usage**: 80-100 MB peak during processing

---

## Architecture Components

### 1. Entry Point: `run_pipe.py`
The main orchestrator that provides comprehensive CLI interface and pipeline coordination.

**Key Responsibilities:**
- Parse command-line arguments with extensive options
- Initialize logging and configuration systems
- Coordinate pipeline execution with dependency management
- Handle force-full-refresh operations
- Provide data analysis and validation capabilities
- Manage log cleanup and maintenance

**Current CLI Options:**
```bash
# Core pipeline execution
--full                    # Run all pipeline stages
--universe               # Process universe data only
--portfolio              # Process portfolio data only
--runs                   # Process runs data (excel + monitor)
--historical-gspread     # Process G-spread analytics

# Force refresh and control
--force-full-refresh     # ðŸ”„ Process ALL raw data ignoring state tracking
--parallel               # Enable parallel execution where possible
--force                  # Continue on errors
--dry-run                # Simulate execution without actual processing

# Data analysis and validation
--analyze-data           # ðŸ“Š Analyze data after pipeline completion
--data-analysis-only     # Only analyze data without running pipeline

# Log management
--cleanup-logs           # ðŸ§¹ Clean logs before running pipeline
--log-cleanup-only       # Only clean logs without running pipeline
--retention-days N       # Configure log retention period (default: 5)

# Configuration and debugging
--config PATH            # Specify custom configuration file
--log-level LEVEL        # Set logging level (DEBUG, INFO, WARNING, ERROR)
--log-file PATH          # Specify custom log file location
```

### 2. Pipeline Manager: `src/orchestrator/pipeline_manager.py`
Central coordination engine with advanced execution planning.

**Core Features:**
- **Dependency Resolution**: Intelligent stage ordering based on data dependencies
- **Parallel Execution**: Concurrent processing where dependencies allow
- **Progress Tracking**: Real-time execution monitoring and reporting
- **Error Recovery**: Retry logic and graceful failure handling
- **Performance Metrics**: Execution time and resource usage tracking

**Pipeline Stages Mapping:**
```python
SCRIPT_MAPPINGS = {
    PipelineStage.UNIVERSE: "universe/universe_raw_to_parquet.py",
    PipelineStage.PORTFOLIO: "portfolio/portfolio_excel_to_parquet.py",
    PipelineStage.HISTORICAL_GSPREAD: "historical g spread/g_z.py",
    PipelineStage.RUNS_EXCEL: "runs/excel_to_df_debug.py",
    PipelineStage.RUNS_MONITOR: "runs/run_monitor.py"
}
```

### 3. Data Processors
Specialized processors for each data type with force-full-refresh support:

#### A. Universe Processor (`src/pipeline/universe_processor.py`)
- **Input**: Excel files in `universe/raw data/`
- **Output**: `universe/universe.parquet` (5.4MB, 31,806 records)
- **Features**: Incremental processing with state tracking, force-full-refresh support
- **State File**: `universe/processing_state.json`

#### B. Portfolio Processor (`src/pipeline/portfolio_processor.py`)  
- **Input**: Excel files in `portfolio/raw data/`
- **Output**: `portfolio/portfolio.parquet` (1.2MB, 6,434 records)
- **Features**: File metadata tracking, duplicate handling, force-full-refresh support
- **State File**: `portfolio/processing_state.json`

#### C. Runs Processors
- **Excel Processor** (`runs/excel_to_df_debug.py`):
  - **Input**: Excel files in `runs/raw/`
  - **Output**: `runs/combined_runs.parquet` (5.9MB, 281,173 records)
  - **Features**: Parallel loading, deduplication, force-full-refresh support
  - **State File**: `runs/last_processed.json`

- **Monitor Processor** (`runs/run_monitor.py`):
  - **Input**: `runs/combined_runs.parquet`
  - **Output**: `runs/run_monitor.parquet` (157KB, 5,830 analysis records)
  - **Features**: Period-over-period analysis, best bid/offer calculation

#### D. G-Spread Analytics (`historical g spread/g_z.py`)
- **Input**: `historical g spread/g_ts.parquet`
- **Output**: `historical g spread/bond_z.parquet` (1.2MB)
- **Features**: Self-contained pairwise Z-score analysis, 11-column output
- **Performance**: 5.75 seconds execution (124,343 records/second)

### 4. Data Analysis System (`src/utils/data_analyzer.py`)
Comprehensive data analysis and validation framework:

**DataAnalyzer Features:**
- **DataFrame Analysis**: `df.info()`, `df.head()`, `df.tail()`, `df.describe()`
- **Time Series Detection**: Automatic date range identification
- **Memory Usage Tracking**: Resource consumption monitoring
- **Null Value Analysis**: Missing data identification
- **Duplicate Detection**: Row duplication analysis

**CUSIPValidator Features:**
- **Cross-Table Validation**: CUSIP consistency across all tables
- **Orphaned CUSIP Detection**: CUSIPs in tables but not in universe
- **Security Name Mapping**: Descriptive names for orphaned CUSIPs
- **Instance Counting**: Total occurrences of each orphaned CUSIP

### 5. Log Management (`src/utils/log_cleanup.py`)
Automated log cleanup and maintenance system:

**Features:**
- **Automatic Cleanup**: Removes logs older than specified retention period
- **Configurable Retention**: Different retention periods for different log types
- **Space Tracking**: Reports disk space freed by cleanup
- **Safe Deletion**: Dry-run mode and detailed reporting

---

## Pipeline Stages

### Stage 1: Universe Processing
**Execution Time**: ~23 seconds  
**Dependencies**: None (entry point)

**Process Flow:**
1. **File Discovery**: Scan `universe/raw data/` for Excel files
2. **State Check**: Compare with `processing_state.json` (unless force-full-refresh)
3. **Data Processing**: Load, clean, and standardize data
4. **Parquet Generation**: Create `universe/universe.parquet`
5. **State Update**: Update processing state with latest dates

**Force-Full-Refresh Behavior:**
- Ignores `processing_state.json`
- Processes ALL Excel files regardless of modification dates
- Updates state file with latest data dates after processing

### Stage 2: Portfolio Processing  
**Execution Time**: ~8 seconds  
**Dependencies**: None (parallel with Universe)

**Process Flow:**
1. **File Metadata**: Analyze Excel files in `portfolio/raw data/`
2. **Change Detection**: Compare metadata with state file (unless force-full-refresh)
3. **Data Processing**: Load and standardize portfolio data
4. **Parquet Generation**: Create `portfolio/portfolio.parquet`
5. **CSV Export**: Generate `portfolio/processed data/portfolio.csv`

**Force-Full-Refresh Behavior:**
- Processes ALL portfolio Excel files
- Ignores existing parquet file for incremental loading
- Creates complete portfolio dataset from scratch

### Stage 3: Runs Processing (Two Sub-stages)

#### Stage 3A: Runs Excel Processing
**Execution Time**: ~53 seconds  
**Dependencies**: None

**Process Flow:**
1. **File Loading**: Parallel loading of Excel files from `runs/raw/`
2. **Data Cleaning**: Remove negative values, deduplicate records
3. **Date/Time Parsing**: Standardize date and time formats
4. **Parquet Generation**: Create `runs/combined_runs.parquet`

**Force-Full-Refresh Behavior:**
- Equivalent to `--force-all` flag behavior
- Processes ALL Excel files regardless of last processed date
- Maintains data quality with deduplication

#### Stage 3B: Runs Monitor Processing
**Execution Time**: ~2 seconds  
**Dependencies**: Runs Excel (requires `combined_runs.parquet`)

**Process Flow:**
1. **Data Loading**: Load `runs/combined_runs.parquet`
2. **Analysis Calculation**: Period-over-period changes (DoD, WoW, MTD, QTD, YTD, 1YR)
3. **Best Levels**: Calculate best bid/offer with size constraints (2M+)
4. **Dealer Attribution**: Identify dealers at best levels
5. **Output Generation**: Create `run_monitor.parquet` and CSV

### Stage 4: G-Spread Analytics
**Execution Time**: ~6 seconds  
**Dependencies**: None (self-contained)

**Process Flow:**
1. **Data Loading**: Load `g_ts.parquet` time series data
2. **Pairwise Analysis**: Calculate Z-scores for bond pairs
3. **Statistical Calculation**: Min, max, percentiles, and spread analysis
4. **Output Generation**: Create `bond_z.parquet` with 11 columns

**Self-Contained Features:**
- No external dependencies (only requires `g_ts.parquet`)
- 11-column output with CUSIP enrichment
- 100% CUSIP match rate for all pairs

---

## Data Flow

### 1. Input Data Sources
```
Raw Data Sources:
â”œâ”€â”€ universe/raw data/              # Market universe Excel files (16 files)
â”œâ”€â”€ portfolio/raw data/             # Portfolio Excel files (14 files) 
â”œâ”€â”€ historical g spread/            # G-spread time series (g_ts.parquet)
â”œâ”€â”€ runs/raw/                       # Trading runs Excel files (22 files)
â””â”€â”€ Various state tracking files    # JSON files for incremental processing
```

### 2. Processing Pipeline
```
Raw Data â†’ State Check â†’ Processors â†’ Parquet Files â†’ Analysis â†’ Validation
    â†“           â†“            â†“            â†“           â†“          â†“
  Excel/     Force-Full-   Cleaning/   Standard    Statistical  CUSIP
  Parquet    Refresh      Validation   Format      Analysis     Cross-ref
```

### 3. Output Structure
```
Processed Data:
â”œâ”€â”€ universe/universe.parquet           (5.4MB, 31,806 records)
â”œâ”€â”€ portfolio/portfolio.parquet         (1.2MB, 6,434 records)
â”œâ”€â”€ historical g spread/bond_z.parquet  (1.2MB, 19,900 pairs)
â”œâ”€â”€ runs/combined_runs.parquet          (5.9MB, 281,173 records)
â””â”€â”€ runs/run_monitor.parquet            (157KB, 5,830 analysis records)
```

### 4. State Tracking Files
```
State Management:
â”œâ”€â”€ universe/processing_state.json     # Universe file metadata and timestamps
â”œâ”€â”€ portfolio/processing_state.json    # Portfolio file metadata and timestamps
â””â”€â”€ runs/last_processed.json           # Last processed date for runs
```

### 5. Analysis Output
```
Analysis Results:
â”œâ”€â”€ DataFrame Analysis Reports          # Memory usage, null analysis, duplicates
â”œâ”€â”€ Statistical Summaries              # Descriptive statistics for all tables
â”œâ”€â”€ CUSIP Validation Results           # Cross-table CUSIP consistency
â”œâ”€â”€ Orphaned CUSIP Analysis            # CUSIPs in tables but not in universe
â””â”€â”€ Time Series Analysis               # Date ranges and data coverage
```

---

## Force Full Refresh System

### ðŸ”„ Overview
The Force Full Refresh system is designed to handle Git version switches where parquet files are lost (they're in `.gitignore`) and need to be completely regenerated from raw data.

### Key Features
- **Complete Regeneration**: Processes ALL raw data regardless of state tracking
- **State Bypassing**: Ignores all JSON state tracking files during processing
- **Fresh Start**: Creates parquet files from scratch without incremental logic
- **State Update**: Updates state files with latest data dates after processing

### Implementation Details

#### Command Line Usage
```bash
# Generate all complete parquet files from scratch
poetry run python run_pipe.py --force-full-refresh

# Generate complete files and then analyze all tables  
poetry run python run_pipe.py --force-full-refresh --analyze-data

# Only specific processors with full refresh
poetry run python run_pipe.py --universe --portfolio --force-full-refresh

# Combine with other options
poetry run python run_pipe.py --force-full-refresh --parallel --cleanup-logs
```

#### Processor-Level Implementation
Each processor supports the `--force-full-refresh` flag:

**Universe Processor:**
```python
def process_universe_files(logger: Logger, force_full_refresh: bool = False):
    if force_full_refresh:
        logger.info("ðŸ”„ FORCE FULL REFRESH: Ignoring state tracking, processing ALL universe files")
        existing_state = {'processed_files': {}, 'last_processed': None}
        # Get all Excel files for processing
        all_files = [f for f in raw_data_path.glob('*.xlsx') if f.is_file()]
        files_to_process = [(f, get_file_metadata(f)) for f in all_files]
```

**Portfolio Processor:**
```python
def process_portfolio_files(logger: Logger, force_full_refresh: bool = False):
    if force_full_refresh:
        logger.info("ðŸ”„ FORCE FULL REFRESH: Ignoring state tracking, processing ALL portfolio files")
        existing_state = {'processed_files': {}, 'last_processed': None}
        # Process all files without existing parquet loading
```

**Runs Processor:**
```python
# In excel_to_df_debug.py
force_all = args.force_all or args.force_full_refresh
if args.force_full_refresh:
    print("[FORCE FULL REFRESH] Processing ALL Excel files regardless of modification date")
```

#### Expected Results
After running `--force-full-refresh`:
1. **All parquet files exist** and are complete
2. **CUSIP validation checks 5 tables** instead of partial tables
3. **State files updated** with latest data dates
4. **Future incremental runs** work correctly
5. **Git version switches** won't break the pipeline

### Use Cases
- **Git Version Switches**: When switching branches/commits where parquet files are missing
- **Data Corruption Recovery**: When parquet files become corrupted
- **Complete Pipeline Reset**: When needing to start fresh
- **Testing**: Validating complete data processing capability
- **Production Deployment**: Initial setup of complete datasets

---

## Configuration Management

### Configuration File: `config/config.yaml`
Centralized configuration for all pipeline components.

**Structure:**
```yaml
logging:
  level: INFO
  format: "[%(asctime)s] %(levelname)s: %(message)s"
  
universe_processor:
  batch_size: 1000
  validation_rules:
    required_columns: ["Date", "CUSIP", "Security"]
    
portfolio_processor:
  deduplication:
    columns: ["Date", "CUSIP", "Security"]
    
runs_processor:
  parallel_workers: 20
  date_format: "%m/%d/%y"
  time_format: "%H:%M"
  
gspread_processor:
  max_bonds: 70
  lookback_days: 250
```

### Environment Variables
```bash
# Optional environment configuration
export PIPELINE_LOG_LEVEL=DEBUG
export PIPELINE_CONFIG_PATH=config/custom_config.yaml
export PIPELINE_PARALLEL_WORKERS=16
```

---

## Execution Orchestration

### Dependency Management
The pipeline manager handles complex dependencies between stages:

```python
DEPENDENCIES = {
    PipelineStage.UNIVERSE: [],                    # No dependencies
    PipelineStage.PORTFOLIO: [],                   # No dependencies  
    PipelineStage.HISTORICAL_GSPREAD: [],          # Self-contained
    PipelineStage.RUNS_EXCEL: [],                  # No dependencies
    PipelineStage.RUNS_MONITOR: [PipelineStage.RUNS_EXCEL]  # Requires combined_runs.parquet
}
```

### Parallel Execution Groups
Stages are grouped for optimal parallel execution:

```
Group 1 (Parallel): [universe, portfolio, historical-gspread, runs-excel]
Group 2 (Sequential): [runs-monitor] (depends on runs-excel completion)
```

### Execution Plan Example
```
PIPELINE EXECUTION PLAN
==================================================
Step 1: portfolio, universe, historical-gspread, runs-excel (parallel, ~3m)
Step 2: runs-monitor (sequential, ~2m)
==================================================
Total estimated time: 0:05:00
Total stages: 5
```

### Progress Tracking
Real-time execution monitoring:
```bash
[RUN] Executing portfolio...
[OK] portfolio completed in 0:00:07.823324
    Records processed: 6,434
    Output files: 1

[RUN] Executing universe...
[OK] universe completed in 0:00:23.497124
    Records processed: 31,806
    Output files: 1
```

---

## Data Processing Details

### Universe Processing
**Input Format**: Excel files with market universe data  
**Key Columns**: Date, CUSIP, Security, G Sprd, OAS (Mid), Z Spread, Rating  
**Processing Steps:**
1. **File Metadata Tracking**: Track modification times and sizes
2. **Incremental Loading**: Only process new/changed files (unless force-full-refresh)
3. **Data Cleaning**: Remove duplicates, standardize formats
4. **Date Parsing**: Handle multiple date formats consistently
5. **Bucketing**: Apply sector and rating bucketing rules from config
6. **Validation**: Ensure required columns and data types
7. **Export**: Generate both parquet and CSV outputs

### Portfolio Processing  
**Input Format**: Excel files with portfolio holdings data  
**Key Columns**: Date, SECURITY, CUSIP, QUANTITY, PRICE, VALUE, VALUE PCT NAV  
**Processing Steps:**
1. **File Change Detection**: Compare with processing state
2. **Data Loading**: Process only modified files (unless force-full-refresh)
3. **Column Standardization**: Normalize column names and types
4. **CUSIP Cleaning**: Remove invalid or blank CUSIPs
5. **Deduplication**: Remove duplicate records based on key columns
6. **Validation**: Check data completeness and consistency
7. **Aggregation**: Combine data from multiple files by date

### Runs Processing
**Input Format**: Excel files with trading runs data  
**Key Columns**: Date, Time, CUSIP, Security, Dealer, Bid/Ask Spread, Bid/Ask Size  
**Processing Steps:**
1. **Parallel Loading**: Load multiple Excel files concurrently (20 workers)
2. **Date/Time Parsing**: Parse dates and times with error handling
3. **Data Cleaning**: Remove negative prices, sizes; filter invalid data
4. **Deduplication**: Remove duplicate records based on key columns
5. **Sorting**: Sort by Date and Time for chronological order
6. **Validation**: Check data integrity and completeness
7. **Most Recent Logic**: For duplicates, keep most recent record

### G-Spread Analytics Processing
**Input Format**: Time series parquet file (`g_ts.parquet`)  
**Key Columns**: Date, CUSIP_1, CUSIP_2, Last_Spread  
**Processing Steps:**
1. **Data Loading**: Load complete time series data
2. **Pairwise Analysis**: Calculate statistics for bond pairs
3. **Z-Score Calculation**: Compute rolling Z-scores and percentiles
4. **Statistical Measures**: Calculate min, max, last vs extremes
5. **CUSIP Enrichment**: Add bond names and identifiers
6. **Self-Contained**: No external dependencies required
7. **High Performance**: 124,343 records/second processing

---

## Analysis & Validation

### Data Analysis System

#### DataFrame Analysis
For each processed table, the system provides:
```bash
ðŸ“Š UNIVERSE ANALYSIS
============================================================
Shape: 31,806 rows Ã— 47 columns
Memory Usage: 13.40 MB
Duplicate Rows: 0
Time Series: YES
  Date: 2023-08-04 to 2025-07-16 (1,462 unique dates)
Columns with nulls: 15
  Benchmark Cusip: 31,806 (100.0%)
  Notes: 31,806 (100.0%)
```

#### Statistical Analysis
Automatic generation of descriptive statistics:
- **Numeric Columns**: Mean, std, min, max, quartiles
- **Text Columns**: Unique values, most frequent values
- **Date Columns**: Date ranges, unique dates, frequency
- **Memory Usage**: Detailed memory consumption per column

### CUSIP Validation System

#### Cross-Table Validation
Validates CUSIP consistency across all tables:
```bash
ðŸ” CUSIP VALIDATION RESULTS
============================================================
Universe CUSIPs: 3,116
Tables Checked: 4
Unique Orphaned CUSIPs: 53
Total Orphaned Instances: 1,991

ðŸ“‹ ORPHANED CUSIPS BY TABLE:
------------------------------------------------------------
ðŸ”´ PORTFOLIO:
   Unique Orphaned CUSIPs: 41
   Total Instances: 496
   â€¢ CDXIG543 - CDXIG543 100 12/29 U (20 instances)
   â€¢ 2933ZWEE7 - ENMAXC 0 05/13/25 CA (2 instances)
```

#### Orphaned CUSIP Detection
Identifies CUSIPs that exist in other tables but not in universe:
- **Portfolio Orphans**: CUSIPs in portfolio but not in universe
- **Runs Orphans**: CUSIPs in trading runs but not in universe  
- **Monitor Orphans**: CUSIPs in run monitor but not in universe
- **G-Spread Orphans**: CUSIPs in analytics but not in universe

#### Data Quality Metrics
- **Coverage Percentages**: How much of universe is covered by each table
- **Instance Counts**: Total occurrences of each orphaned CUSIP
- **Security Name Mapping**: Descriptive names for easy identification
- **Table Attribution**: Which specific tables contain orphaned CUSIPs

---

## Error Handling & Recovery

### Comprehensive Error Handling

#### File-Level Error Handling
- **Missing Files**: Graceful handling of missing input files
- **Corrupted Files**: Detection and reporting of corrupted Excel/parquet files
- **Permission Errors**: Clear messaging for file access issues
- **Path Resolution**: Automatic path detection for different execution contexts

#### Data-Level Error Handling
- **Invalid Data Types**: Automatic type conversion with fallbacks
- **Missing Columns**: Clear reporting of required vs available columns
- **Date Parsing Errors**: Multiple date format attempts with error logging
- **CUSIP Standardization**: Safe CUSIP processing with fallback to original

#### System-Level Error Handling
- **Memory Issues**: Garbage collection and memory optimization
- **Unicode Encoding**: Windows console encoding compatibility
- **Log File Locking**: Delayed file opening to prevent Windows locking issues
- **Empty DataFrames**: Safe handling of empty datasets

### Recovery Mechanisms

#### Retry Logic
- **File Loading**: Automatic retry for transient file access issues
- **Network Operations**: Retry logic for any external dependencies
- **Database Operations**: Transaction rollback and retry capabilities

#### Graceful Degradation
- **Partial Processing**: Continue processing other stages if one fails
- **Force Continue**: `--force` flag to continue despite errors
- **Dry Run**: `--dry-run` to validate without execution
- **Rollback**: Ability to restore previous state on critical failures

#### Error Reporting
- **Detailed Logging**: Comprehensive error context and stack traces
- **User-Friendly Messages**: Clear guidance for common issues
- **Progress Preservation**: Resume capability for long-running operations
- **Status Reporting**: Clear indication of what succeeded vs failed

---

## Monitoring & Logging

### Logging System

#### Log Configuration
**Default Locations:**
- **Pipeline Orchestrator**: `logs/pipeline_orchestrator_YYYYMMDD_HHMMSS.log`
- **Universe Processor**: `logs/universe_processor.log`
- **Portfolio Processor**: `logs/portfolio_processor.log`
- **Runs Processors**: `logs/` directory with rotating files

**Log Levels:**
- **DEBUG**: Detailed execution flow and variable values
- **INFO**: Standard operations and progress updates
- **WARNING**: Non-critical issues and fallback behaviors
- **ERROR**: Failures and exceptions with context

#### Log Rotation and Cleanup
```python
# Automatic cleanup configuration
LogCleanupManager(
    logs_dir="logs", 
    retention_days=5,  # Configurable retention period
    dry_run=False      # Preview mode available
)
```

**Cleanup Features:**
- **Automatic Cleanup**: Remove logs older than retention period
- **Space Tracking**: Report disk space freed
- **Safe Deletion**: Dry-run mode for preview
- **Configurable Retention**: Different periods for different log types

### Performance Monitoring

#### Execution Metrics
```bash
ðŸš€ PERFORMANCE METRICS:
   â±ï¸  Total Duration: 4.6 minutes
   ðŸ“ˆ Records Processed: 188,398
   âš¡ Processing Rate: 7,558 records/second
   ðŸ’¾ Memory Usage: 80-100 MB peak
```

#### Stage-Level Monitoring
Each pipeline stage reports:
- **Execution Time**: Start/end timestamps and duration
- **Records Processed**: Input/output record counts
- **File Operations**: Files read/written with sizes
- **Memory Usage**: Peak memory consumption
- **Error Counts**: Number of errors/warnings encountered

#### Resource Monitoring
- **Disk Space**: Input/output file sizes and disk usage
- **Memory Consumption**: Peak and average memory usage
- **CPU Utilization**: Processing efficiency metrics
- **I/O Operations**: File read/write performance

---

## CLI Interface

### Complete Command Reference

#### Core Pipeline Operations
```bash
# Run complete pipeline with all stages
poetry run python run_pipe.py --full

# Run specific stages
poetry run python run_pipe.py --universe
poetry run python run_pipe.py --portfolio  
poetry run python run_pipe.py --runs
poetry run python run_pipe.py --historical-gspread

# Multiple stages
poetry run python run_pipe.py --universe --portfolio --runs
```

#### Force Full Refresh Operations
```bash
# ðŸ”„ Complete regeneration of all parquet files
poetry run python run_pipe.py --force-full-refresh

# Force refresh specific stages
poetry run python run_pipe.py --force-full-refresh --universe --portfolio

# Force refresh with analysis
poetry run python run_pipe.py --force-full-refresh --analyze-data

# Force refresh with parallel processing
poetry run python run_pipe.py --force-full-refresh --parallel
```

#### Data Analysis Operations
```bash
# ðŸ“Š Analyze data after pipeline completion
poetry run python run_pipe.py --full --analyze-data

# Only analyze data without running pipeline
poetry run python run_pipe.py --data-analysis-only

# Analyze specific processed files
poetry run python run_pipe.py --universe --analyze-data
```

#### Log Management Operations
```bash
# ðŸ§¹ Clean logs before running pipeline
poetry run python run_pipe.py --full --cleanup-logs

# Only clean logs without running pipeline
poetry run python run_pipe.py --log-cleanup-only

# Clean with custom retention period
poetry run python run_pipe.py --log-cleanup-only --retention-days 3
```

#### Advanced Control Options
```bash
# Execution control
--parallel               # Enable parallel execution where possible
--force                  # Continue execution despite errors
--dry-run                # Simulate execution without actual processing

# Configuration
--config PATH            # Specify custom configuration file
--log-level LEVEL        # Set logging level (DEBUG, INFO, WARNING, ERROR)  
--log-file PATH          # Specify custom log file location

# Resume and validation
--resume-from STAGE      # Resume execution from specific stage
--validate-only          # Only validate configuration and files
```

### Usage Examples

#### Production Deployment
```bash
# Complete production pipeline with all optimizations
poetry run python run_pipe.py --full --parallel --cleanup-logs --analyze-data
```

#### Development and Testing  
```bash
# Test configuration without execution
poetry run python run_pipe.py --dry-run --validate-only

# Debug specific stage with detailed logging
poetry run python run_pipe.py --universe --log-level DEBUG

# Analyze existing data for debugging
poetry run python run_pipe.py --data-analysis-only
```

#### Data Recovery and Reset
```bash
# Complete reset after Git version switch
poetry run python run_pipe.py --force-full-refresh --full

# Recover from data corruption
poetry run python run_pipe.py --force-full-refresh --universe --portfolio

# Validate data quality after recovery
poetry run python run_pipe.py --force-full-refresh --analyze-data
```

#### Maintenance Operations
```bash
# Regular maintenance with log cleanup
poetry run python run_pipe.py --log-cleanup-only --retention-days 7

# Check pipeline status without execution
poetry run python run_pipe.py --validate-only

# Performance testing with timing
poetry run python run_pipe.py --full --parallel
```

---

## File Structure

### Project Directory Structure
```
work_supa/
â”œâ”€â”€ ðŸ“ ai_instructions/               # AI and automation instructions
â”œâ”€â”€ ðŸ“ change logs/                   # Project change tracking
â”œâ”€â”€ ðŸ“ config/                        # Configuration files
â”‚   â””â”€â”€ config.yaml                   # Main configuration
â”œâ”€â”€ ðŸ“ db/                           # Database files (if used)
â”œâ”€â”€ ðŸ“ docs/                         # Documentation
â”‚   â”œâ”€â”€ ARCHITECTURE.md
â”‚   â”œâ”€â”€ PROJECT_SUMMARY.md
â”‚   â”œâ”€â”€ QUICK_REFERENCE.md
â”‚   â””â”€â”€ pipe.md
â”œâ”€â”€ ðŸ“ historical g spread/           # G-spread analytics
â”‚   â”œâ”€â”€ ðŸ“ raw data/                 # Time series data
â”‚   â”œâ”€â”€ ðŸ“ processed data/           # Analytics outputs
â”‚   â”œâ”€â”€ g_ts.parquet                 # Input time series
â”‚   â”œâ”€â”€ bond_z.parquet               # Analytics output (1.2MB)
â”‚   â””â”€â”€ g_z.py                       # Analytics processor
â”œâ”€â”€ ðŸ“ logs/                         # System logs (auto-managed)
â”œâ”€â”€ ðŸ“ portfolio/                    # Portfolio data
â”‚   â”œâ”€â”€ ðŸ“ raw data/                 # Portfolio Excel files (14 files)
â”‚   â”œâ”€â”€ ðŸ“ processed data/           # CSV exports
â”‚   â”œâ”€â”€ portfolio.parquet            # Main output (1.2MB)
â”‚   â”œâ”€â”€ portfolio_excel_to_parquet.py
â”‚   â””â”€â”€ processing_state.json        # State tracking
â”œâ”€â”€ ðŸ“ runs/                         # Trading runs data
â”‚   â”œâ”€â”€ ðŸ“ raw/                      # Runs Excel files (22 files)
â”‚   â”œâ”€â”€ ðŸ“ processed runs data/      # CSV exports
â”‚   â”œâ”€â”€ combined_runs.parquet        # Combined data (5.9MB)
â”‚   â”œâ”€â”€ run_monitor.parquet          # Analysis data (157KB)
â”‚   â”œâ”€â”€ excel_to_df_debug.py         # Excel processor
â”‚   â”œâ”€â”€ run_monitor.py               # Monitor processor
â”‚   â””â”€â”€ last_processed.json          # State tracking
â”œâ”€â”€ ðŸ“ src/                          # Source code
â”‚   â”œâ”€â”€ ðŸ“ orchestrator/             # Pipeline orchestration
â”‚   â”‚   â”œâ”€â”€ pipeline_config.py
â”‚   â”‚   â””â”€â”€ pipeline_manager.py
â”‚   â”œâ”€â”€ ðŸ“ pipeline/                 # Data processors
â”‚   â”‚   â”œâ”€â”€ universe_processor.py
â”‚   â”‚   â”œâ”€â”€ portfolio_processor.py
â”‚   â”‚   â””â”€â”€ [other processors]
â”‚   â””â”€â”€ ðŸ“ utils/                    # Utilities
â”‚       â”œâ”€â”€ data_analyzer.py         # Data analysis system
â”‚       â”œâ”€â”€ expert_logging.py        # Logging system
â”‚       â”œâ”€â”€ log_cleanup.py           # Log management
â”‚       â””â”€â”€ [other utilities]
â”œâ”€â”€ ðŸ“ test/                         # Test suite
â”œâ”€â”€ ðŸ“ universe/                     # Market universe data
â”‚   â”œâ”€â”€ ðŸ“ raw data/                 # Universe Excel files (16 files)
â”‚   â”œâ”€â”€ ðŸ“ processed data/           # CSV exports
â”‚   â”œâ”€â”€ universe.parquet             # Main output (5.4MB)
â”‚   â”œâ”€â”€ universe_raw_to_parquet.py
â”‚   â””â”€â”€ processing_state.json        # State tracking
â”œâ”€â”€ run_pipe.py                      # ðŸŽ¯ Main entry point
â”œâ”€â”€ cleanup_logs.py                  # Standalone log cleanup
â”œâ”€â”€ pyproject.toml                   # Python project configuration
â””â”€â”€ README.md                        # Project documentation
```

### Key Parquet Files Generated
```
ðŸ“Š Primary Outputs (Complete after force-full-refresh):
â”œâ”€â”€ universe/universe.parquet          (5.4MB, 31,806 records)
â”œâ”€â”€ portfolio/portfolio.parquet        (1.2MB, 6,434 records)
â”œâ”€â”€ runs/combined_runs.parquet         (5.9MB, 281,173 records)
â”œâ”€â”€ runs/run_monitor.parquet           (157KB, 5,830 analysis records)
â””â”€â”€ historical g spread/bond_z.parquet (1.2MB, 19,900 pairs)
```

### State Tracking Files
```
ðŸ”„ State Management (Bypassed during force-full-refresh):
â”œâ”€â”€ universe/processing_state.json     # File metadata and timestamps
â”œâ”€â”€ portfolio/processing_state.json    # File metadata and timestamps
â””â”€â”€ runs/last_processed.json           # Last processed date
```

### Log Files
```
ðŸ“ Logging Outputs (Auto-managed with cleanup):
â”œâ”€â”€ logs/pipeline_orchestrator_YYYYMMDD_HHMMSS.log
â”œâ”€â”€ logs/universe_processor.log
â”œâ”€â”€ logs/portfolio_processor.log
â””â”€â”€ [rotating logs with 5-day retention]
```

---

## Performance & Optimization

### Current Performance Metrics
- **Processing Rate**: 7,558 records/second (optimized)
- **Pipeline Duration**: 4-6 minutes for complete processing
- **Memory Usage**: 80-100 MB peak during processing
- **Parallel Efficiency**: 3-4x speedup with parallel execution
- **Data Throughput**: ~10MB/minute sustained

### Optimization Features

#### Force Full Refresh Optimizations
- **Parallel File Processing**: Multiple Excel files processed concurrently
- **Memory Management**: Efficient DataFrame operations and cleanup
- **State Bypassing**: Skip state file checks for maximum speed
- **Batch Processing**: Optimized batch sizes for large datasets

#### Pipeline Optimizations
- **Dependency Resolution**: Intelligent execution ordering
- **Parallel Execution**: Concurrent processing where dependencies allow
- **Memory Optimization**: Garbage collection and efficient data structures
- **I/O Optimization**: Optimized file reading and writing

#### Performance Monitoring
```bash
ðŸš€ PERFORMANCE METRICS:
   â±ï¸  Total Duration: 4.6 minutes
   ðŸ“ˆ Records Processed: 188,398
   âš¡ Processing Rate: 7,558 records/second
   ðŸ’¾ Memory Usage: 80-100 MB peak
   ðŸ“Š Parallel Efficiency: 3.2x speedup
```

### Resource Requirements
- **Memory**: 2-4 GB RAM recommended (512MB minimum)
- **Disk Space**: 2-5 GB for complete dataset
- **CPU**: Multi-core recommended for parallel processing
- **Network**: Not required (local processing only)

### Scalability Features
- **Configurable Workers**: Adjustable parallelism based on system resources
- **Chunked Processing**: Large files processed in manageable chunks
- **Incremental Processing**: Only process changed data (unless force-full-refresh)
- **Memory Management**: Automatic cleanup and optimization

---

## Dependencies & Relationships

### System Dependencies
```
Python 3.11+
â”œâ”€â”€ pandas >= 2.0.0              # Data processing
â”œâ”€â”€ pyarrow >= 10.0.0            # Parquet file support
â”œâ”€â”€ openpyxl >= 3.0.0            # Excel file processing
â”œâ”€â”€ pyyaml >= 6.0                # Configuration management
â”œâ”€â”€ pathlib                      # Path handling
â”œâ”€â”€ concurrent.futures           # Parallel processing
â”œâ”€â”€ logging                      # System logging
â””â”€â”€ argparse                     # CLI interface
```

### Data Dependencies
```
Input Dependencies:
â”œâ”€â”€ universe/raw data/           # Excel files (required for universe processing)
â”œâ”€â”€ portfolio/raw data/          # Excel files (required for portfolio processing)
â”œâ”€â”€ runs/raw/                    # Excel files (required for runs processing)
â””â”€â”€ historical g spread/         # g_ts.parquet (required for analytics)

State Dependencies (bypassed with --force-full-refresh):
â”œâ”€â”€ universe/processing_state.json
â”œâ”€â”€ portfolio/processing_state.json
â””â”€â”€ runs/last_processed.json
```

### Processing Dependencies
```
Stage Dependencies:
â”œâ”€â”€ universe â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ (independent)
â”œâ”€â”€ portfolio â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ (independent)
â”œâ”€â”€ historical-gspread â”€â”€â”€â”€â”€â”€ (independent, self-contained)
â”œâ”€â”€ runs-excel â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ (independent)
â””â”€â”€ runs-monitor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ depends on: runs-excel (combined_runs.parquet)
```

### Output Dependencies
```
Analysis Dependencies:
â”œâ”€â”€ Data Analysis â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ depends on: all parquet files
â”œâ”€â”€ CUSIP Validation â”€â”€â”€â”€â”€â”€â”€â”€â”€ depends on: universe.parquet + other tables
â””â”€â”€ Orphaned CUSIP Detection â”€â”€ depends on: universe.parquet + other tables
```

---

## Troubleshooting Guide

### Common Issues and Solutions

#### Force Full Refresh Issues
```
Issue: Parquet files missing after Git version switch
Solution: poetry run python run_pipe.py --force-full-refresh

Issue: Partial CUSIP validation (only 2 tables checked)
Solution: Regenerate all parquet files with --force-full-refresh

Issue: State tracking files causing inconsistent behavior
Solution: Use --force-full-refresh to bypass state tracking
```

#### Performance Issues
```
Issue: Slow processing with large datasets
Solution: Enable parallel processing with --parallel flag

Issue: Memory usage too high
Solution: Process stages individually instead of --full

Issue: Unicode encoding errors on Windows
Solution: Fixed in current version (replaced emoji characters)
```

#### Data Quality Issues
```
Issue: Orphaned CUSIPs detected
Solution: Run --analyze-data to identify specific orphaned CUSIPs

Issue: Missing data in processed files
Solution: Use --force-full-refresh to ensure complete processing

Issue: Date parsing errors
Solution: Check date formats in configuration file
```

#### Configuration Issues
```
Issue: Configuration file not found
Solution: Verify config/config.yaml exists or use --config PATH

Issue: Missing dependencies
Solution: Run poetry install to install all dependencies

Issue: Permission errors
Solution: Check file permissions for input/output directories
```

### Advanced Troubleshooting

#### Debug Mode
```bash
# Enable detailed debugging
poetry run python run_pipe.py --log-level DEBUG --universe

# Dry run to test configuration
poetry run python run_pipe.py --dry-run --validate-only

# Analyze existing data without processing
poetry run python run_pipe.py --data-analysis-only
```

#### Recovery Procedures
```bash
# Complete system reset
poetry run python run_pipe.py --force-full-refresh --full --cleanup-logs

# Validate data integrity
poetry run python run_pipe.py --analyze-data

# Check system status
poetry run python run_pipe.py --validate-only
```

---

## Security & Compliance

### Data Security
- **Local Processing**: All data processed locally, no external transmission
- **File Permissions**: Proper file system permissions for data directories
- **Log Security**: Sensitive data not logged or masked appropriately
- **Git Security**: Database and parquet files excluded from version control

### Audit Trail
- **Comprehensive Logging**: All operations logged with timestamps
- **Execution Tracking**: Stage-by-stage execution records with results
- **Error Documentation**: Detailed error context and recovery actions
- **Performance Metrics**: Execution time and resource usage tracking

### Data Integrity
- **Multi-Level Validation**: Schema validation, data type checking, business rules
- **CUSIP Validation**: Cross-table CUSIP consistency checking
- **Checksums**: Data integrity verification for critical files
- **Backup**: Original data preserved during processing
- **Version Control**: Configuration versioning and change tracking

---

## Future Enhancements

### Planned Features
1. **Real-time Processing**: Stream processing capabilities for live data
2. **Advanced Analytics**: Machine learning and predictive analytics
3. **Web Dashboard**: Interactive monitoring and control interface
4. **API Integration**: REST API for external system integration
5. **Cloud Deployment**: Containerized deployment for cloud platforms

### Performance Enhancements
1. **Distributed Processing**: Spark-based distributed computing
2. **Caching Layer**: Redis-based caching for improved performance
3. **Database Integration**: Direct database connectivity for real-time data
4. **Streaming**: Kafka-based streaming data processing
5. **GPU Acceleration**: CUDA-based acceleration for numerical computations

### Monitoring Enhancements
1. **Metrics Dashboard**: Grafana-based monitoring dashboard
2. **Alerting System**: Automated alerting for failures and performance issues
3. **Data Lineage**: Complete data lineage tracking and visualization
4. **Quality Metrics**: Automated data quality scoring and reporting
5. **Capacity Planning**: Predictive capacity planning and resource optimization

---

## Conclusion

The Trading Analytics Pipeline System represents a comprehensive, production-ready solution for financial data processing and analysis. With the addition of the Force Full Refresh capability, the system now handles Git version switches seamlessly while maintaining high performance, comprehensive validation, and robust error handling.

### Key Strengths
- âœ… **Complete Data Processing**: End-to-end pipeline for all data sources
- âœ… **Force Full Refresh**: Seamless recovery from Git version switches  
- âœ… **High Performance**: 7,558 records/second processing rate
- âœ… **Comprehensive Analysis**: DataFrame analysis and CUSIP validation
- âœ… **Robust Error Handling**: Graceful degradation and recovery
- âœ… **Production Ready**: Complete logging, monitoring, and maintenance

### System Capabilities Summary
- **188,398+ records processed** across all data sources
- **5 parquet files generated** with complete datasets
- **4-6 minute pipeline execution** for full processing
- **Cross-table CUSIP validation** with orphaned CUSIP detection
- **Automatic log management** with configurable retention
- **Force full refresh** for complete data regeneration

The system is now fully operational and ready for production deployment with confidence in data quality, performance, and reliability.

---

*Documentation Version: 2.5*  
*Last Updated: 2025-07-19*  
*Pipeline Version: 2.5.0 with Force Full Refresh Enhancement* 